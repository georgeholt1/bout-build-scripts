INTRO_MPI(3)                                                                                                                                                                                                      INTRO_MPI(3)

NAME
       intro_mpi - Introduces the Message Passing Interface (MPI)

DESCRIPTION
       The Message-Passing Interface (MPI) supports parallel programming across a network of computer systems through a technique known as message passing. The goal of the MPI Forum, simply stated, is to develop a widely
       used standard for writing message-passing programs. MPI establishes a practical, portable, efficient, and flexible standard for message passing that makes use of the most attractive features of a number of existing
       message passing systems, rather than selecting one of them and adopting it as the standard.

       MPI is a specification (like C or Fortran) that has a number of implementations.

       Other sources of MPI information include the man pages for MPI library functions and the following URLs:

           http://www.mpich.org
           http://www.mpi-forum.org

       The default netmod that HPE Cray MPI uses is libfabric (OFI). Libfabric is an open source project as a subgroup of the OpenFabrics Alliance. For more information visit:

       ... http://ofiwg.github.io/libfabric http://www.openfabrics.org ...

       The MPI library explicitly sets default values for a subset of OFI environment variables that have been tested and determined appropriate. These are documented below, starting with the prefix FI_.

           Note
           Changing the values of OFI environment variables to non-default values may lead to undefined behavior. The OFI environment variables are mostly designed for advanced users, or for specific tunings or workarounds
           recommended by HPE.

       Cray-MPICH also provides UCX as a non-default netmod. To use the UCX netmod, module swap to the Cray-MPICH-UCX module. Once loaded, the intro_mpi manpage will contain information specific to the UCX netmod
       implementation.

   About the MPI Module
       The MPI module is not loaded by default. Users must either load the module manually or have their system administrator select system-wide default preferences in /etc/bash.bashrc.local and /etc/csh.cshrc.local.
       Therefore, if your code uses MPI, verify that the cray-mpich module is loaded before compiling or linking. This ensures that your code is linked using the -lmpich option.

   GPU Support in Cray MPICH
       Cray MPICH for HPE Cray EX systems supports GPU devices. Currently, NVIDIA and AMD GPU devices are supported. The Cray MPICH software stack works in collaboration with the rest of the Cray PE stack to offer GPU
       support.

       Cray MPI offers "GPU Aware" MPI support for applications that perform MPI operations with communication buffers that are on GPU-attached memory regions. Cray MPI supports the following technologies for MPI
       operations involving GPU-attached memory regions:

       · GPU-NIC RDMA (for inter-node MPI transfers)

       · GPU Peer2Peer IPC (for intra-node MPI transfers)

       The following sections include sample recipes for compiling, linking, and running GPU-enabled parallel applications on Shasta systems:

       Section 1 includes the details for Shasta v1.4 systems with NVIDIA GPUS.

       Section 2 includes the relevant details for Shasta v1.4 systems with AMD GPUs.

       Section 1. Using HPE Cray MPI’s GPU support on Shasta v1.4 systems with NVIDIA GPUs:

       NVIDIA GPU support in Cray MPI is available for usage models involving PrgEnv-nvidia, PrgEnv-cray, and PrgEnv-gnu flavors. However, there are subtle variations in the level of GPU support offered for each PrgEnv
       flavor.

       PrgEnv Support
           Cray PE will provide PrgEnv-nvidia for CPU-only applications, CPU/GPU applications with CUDA and CPU codes in the same file, and CPU/GPU applications that use OpenMP offload.

           Cray PE will provide PrgEnv-cray for CPU-only applications and CPU/GPU applications with CUDA and CPU codes in different files. Cray (CC, ftn, cc) drivers must be used at link time and CUDA runtime must be
           included.

           Cray PE will provide PrgEnv-gnu for CPU-only applications, CPU/GPU applications with CUDA and CPU codes in the same file. Nvcc host compiler and gcc versions need to be compatible. (nvcc currently supports GCC
           9.x). Mixing Fortran and CUDA in the same source file is not supported in gfortran CPU/GPU applications that use OpenMP offload are not supported.

       Compiling and linking examples
           Section 1 (a). Compiling and linking NVIDIA GPU-enabled parallel applications on a Shasta v1.4 system:

           This section describes a sample recipe for compiling and linking GPU-enabled parallel applications on a SlingShot-10 system with the Shasta v1.4 software stack with NVIDIA GPUs.

           For the purpose of this illustration, the SlingShot-10 system consists of compute nodes that are based on NVIDIA GPUs and Mellanox RoCE NICs. The Shasta v1.4 stack supports the NVIDIA cudatoolkit version 11.0.

               $module load PrgEnv-cray

               $module load cray-mpich

               $module load cudatoolkit

               $module load craype-accel-nvidia70

               For the  release of Cray MPI on a Shasta v1.4 system, the following
               linker options need to be added by an end user to enable the NVIDIA GDRCopy
               optimization.
               "-L/usr/lib64 -lgdrapi"

               "-lgdrapi" is the NVIDIA GDRCopy library. This library is packaged and released
               along with the rest of the Shasta software stack. Cray MPI relies on the GDRCopy
               layer to optimize small message intra-node and inter-node MPI transfers
               involving communication buffers that are resident on NVIDIA GPU devices.
               GDRCopy is an optional layer, but is strongly recommended since HPE Cray MPI
               relies on GDRCopy to improve small message MPI communication performance.

               Upcoming versions of Cray PE and Cray Shasta software stacks will simplify
               this process by automatically updating the linker flags and the CRAY_LD_LIBRARY_PATH
               variable based on the set of modules that have been loaded.

               Section 1 (b). Running NVIDIA GPU-enabled parallel applications on a Shasta v1.4 system:

               The following is a simple recipe for running GPU-enabled parallel applications
               on a SlingShot-10 system with NVIDIA GPUs. This example assumes the use of 4
               GPUs per node across 64 compute nodes.

               Cray MPI will neither select a default NVIDIA GPU device for a given process, nor
               initialize a default CUDA context for that process. The intent is to allow users
               retain the flexibility of establishing the process-to-CPU and the process-to-GPU
               mapping to match an application's requirements.

               An end user can establish process-to-CPU mapping via slurm runtime options.
               There are two recommended ways to establish process-to-GPU mapping:

               ·   CUDA_VISIBLE_DEVICES:

               NVIDIA offers the CUDA_VISIBLE_DEVICES environment variable to limit the
               number of GPU devices that are available for a given process. A simple way of initializing
               CUDA_VISIBLE_DEVICES is to use the SLURM_LOCALID variable. The following snippet
               illustrates this example:

           $cat select_gpu_device #!/bin/bash

           export CUDA_VISIBLE_DEVICES=$SLURM_LOCALID

           exec $* $export MPICH_GPU_SUPPORT_ENABLED=1

           $srun -p <GPU_Partition> -n256 -N64 --ntasks-per-node=4 --cpu-bind=map_cpu:0,16,32,48 ./select_gpu_device ./exe

           Assuming the compute node has four NVIDIA GPUs, each MPI rank detects the availability of a single GPU device. However, each rank is essentially associated with a different GPU device to ensure that two MPI
           ranks do not share the same GPU device. Hence, for each MPI rank, the default CUDA context is initialized by the GPU runtime layer only on the GPU device that is visible to it.

           Users have the flexibility of initializing CUDA_VISIBLE_DEVICES via other mechanisms.

           · CUDA APIs:

           An alternative approach involves the use of explicit CUDA API calls to detect the number of GPU devices and bind each process to a specific GPU device. In order to perform this mapping each MPI process needs to
           be aware of its "local id" within a compute node. If an application chooses to establish the process-to-GPU mapping before MPI_Init, the $SLURM_LOCALID variable can be used.

           Section 2. Using HPE Cray MPI’s GPU support on Shasta v1.4 systems with AMD GPUs: AMD GPU support in Cray MPI is available for usage models involving PrgEnv-amd, PrgEnv-cray, and PrgEnv-gnu flavors.

           For AMD GPUs, each of these PrgEnv flavors can be used for CPU-only applications, CPU/GPU applications with ROCm and CPU codes in the same file, and CPU/GPU applications that use OpenMP offload

           Section 2 (a). Compiling and linking AMD GPU-enabled parallel applications on Shasta v1.4 systems:

           This section describes a sample recipe for compiling and linking GPU-enabled parallel applications on a SlingShot-10 system with the Shasta v1.4 software stack for AMD GPUs.

           For the purpose of this illustration, the SlingShot-10 system consists of compute nodes that are based on AMD GPUs and Mellanox RoCE NICs. The Shasta v1.4 stack supports the AMD ROCm version 4.1.0.

           $module load PrgEnv-cray

           $module load cray-mpich

           $module load rocm/4.1.0

           $module load craype-accel-amd-gfx908

           Section 2 (b). Running AMD GPU-enabled parallel applications on a Shasta v1.4 system:

           The following is a simple recipe for running GPU-enabled parallel applications on a SlingShot-10 system with AMD GPUs. This example assumes the use of 4 GPUs per node across 64 compute nodes.

           Cray MPI will neither select a default GPU device for a given process, nor initialize a default AMD GPU context for that process. The intent is to allow users retain the flexibility of establishing the
           process-to-CPU and the process-to-GPU mapping to match an application’s requirements.

           An end user can establish process-to-CPU mapping via slurm runtime options. There are two recommended ways to establish process-to-GPU mapping:

           · ROCR_VISIBLE_DEVICES: AMD offers the ROCR_VISIBLE_DEVICES environment variable to limit the number of GPU devices that are available for a given process. A simple way of initializing ROCR_VISIBLE_DEVICES is to
           use the SLURM_LOCALID variable. The following snippet illustrates this example:

           $cat select_gpu_device

           #!/bin/bash

           export ROCR_VISIBLE_DEVICES=$SLURM_LOCALID

           exec $*

           $srun -p <GPU_Partition> -n 256 -N64 --ntasks-per-node=4 --cpu-bind=map_cpu:0,16,32,48 ./select_gpu_device ./exe

           Assuming the compute node has four AMD GPUs, each MPI rank detects the availability of a single GPU device. However, each rank is essentially associated with a different GPU device to ensure that two MPI ranks
           do not share the same GPU device. Hence, for each MPI rank, the default GPU context is initialized by the GPU runtime layer only on the GPU device that is visible to it.

           Users have the flexibility of initializing ROCR_VISIBLE_DEVICES via other mechanisms.

           · HIP (ROCm) APIs: An alternative approach involves the use of explicit HIP API calls to detect the number of GPU devices and bind each process to a specific GPU device. In order to perform this mapping each MPI
           process needs to be aware of its "local id" within a compute node. If an application chooses to establish the process-to-GPU mapping before MPI_Init, the $SLURM_LOCALID variable can be used.

           Section 3. Mapping processes to network interfaces:

           Mapping processes to network interfaces
               On compute nodes that offer multiple GPU devices and multiple Network Interface Controllers (NIC), Cray MPI offers a flexible way to offer the ideal mapping between a process and the default NIC.

               For GPU-enabled parallel applications that involve MPI operations that access application arrays resident are on GPU-attached memory regions, users can set MPICH_OFI_NIC_POLICY to GPU. In this case, for each
               MPI process, Cray MPI strives to select a NIC device that is closest to the GPU device being used.

               For CPU-enabled applications, or for GPU-enabled applications that do not involve MPI operations that access GPU-attached memory regions, users can set MPICH_OFI_NIC_POLICY to NUMA. In this case, for each
               MPI process, Cray MPI strives to select a NIC device that is closest to the CPU NUMA domain being used.

               For custom NIC selection requirements, please refer to the section on MPICH_OFI_NIC_POLICY in this man page.

ENVIRONMENT
       Environment variables have predefined values. You can change some variables to achieve particular performance objectives; others are required values for standard-compliant programs.

   GENERAL MPICH ENVIRONMENT VARIABLES
       MPICH_ABORT_ON_ERROR
           If enabled, causes MPICH to abort and produce a core dump when MPICH detects an internal error. Note that the core dump size limit (usually 0 bytes by default) must be reset to an appropriate value in order to
           enable coredumps.

           Default: Not enabled.

       MPICH_ASYNC_PROGRESS
           If enabled, MPICH will initiate an additional thread to make asynchronous progress on all communication operations including point-to-point, collective, one-sided operations, and I/O. Setting this variable will
           automatically increase the thread-safety level to MPI_THREAD_MULTIPLE. While this improves the progress semantics, it might cause a small amount of performance overhead for regular MPI operations. The user is
           encouraged to leave one or more hardware threads vacant in order to prevent contention between the application threads and the progress thread(s). The impact of oversubscription is highly system dependent but
           may be substantial in some cases, hence this recommendation.

           Default: Not enabled.

       MPICH_COLL_SYNC
           If enabled, a Barrier is performed at the beginning of each specified MPI collective function. This forces all processes participating in that collective to sync up before the collective can begin.

           To disable this feature for all MPI collectives, set the value to 0. This is the default.

           To enable this feature for all MPI collectives, set the value to 1.

           To enable this feature for selected MPI collectives, set the value to a comma-separated list of the desired collective names. Names are not case-sensitive. Any unrecognizable name is flagged with a warning
           message and ignored. The following collective names are recognized: MPI_Allgather, MPI_Allgatherv, MPI_Allreduce, MPI_Alltoall, MPI_Alltoallv, MPI_Alltoallw, MPI_Bcast, MPI_Exscan, MPI_Gather, MPI_Gatherv,
           MPI_Reduce, MPI_Reduce_scatter, MPI_Scan, MPI_Scatter, and MPI_Scatterv.

           Default: Not enabled.

       MPICH_ENV_DISPLAY
           If set, causes rank 0 to display all MPICH environment variables and their current settings at MPI initialization time.

           Default: Not enabled.

       MPICH_GPU_SUPPORT_ENABLED
           If set to 1, enables GPU support. Currently, AMD and NVIDIA GPUs are supported. If a parallel application is GPU-enabled and performs MPI operations with communication buffers that are on GPU-attached memory
           regions, MPICH_GPU_SUPPORT_ENABLED needs to be set to 1.

           Default: 0

       MPICH_GPU_IPC_ENABLED
           If set to 1, enables GPU IPC support for intra-node GPU-GPU communication operations. Currently, this supports the use of IPC for both AMD and NVIDIA GPUs. If MPICH_GPU_SUPPORT_ENABLED is set to 1,
           MPICH_GPU_IPC_ENABLED is automatically set to 1. This variable has no effect if MPICH_GPU_SUPPORT_ENABLED is set to 0

           Default: 1

       MPICH_GPU_EAGER_REGISTER_HOST_MEM
           If set to 1, the MPI library registers the CPU-attached shared memory regions with the GPU runtime layers. These shared memory regions are used for small message intra-node CPU-to-GPU and GPU-to-GPU MPI
           transfers. This optimization helps amortize the cost of registering memory with the GPU runtime layer.  MPICH_GPU_EAGER_REGISTER_HOST_MEM is automatically set to 1, if MPICH_GPU_SUPPORT_ENABLED is set to 1. This
           variable has no effect if MPICH_GPU_SUPPORT_ENABLED is set to 0

           Default: 1

       MPICH_GPU_IPC_THRESHOLD
           This variable determines the threshold for the GPU IPC capability. GPU IPC takes advantage of DMA engines on GPU devices to accelerate data movement operations between GPU devices on the same node. Intra-node
           GPU-GPU transfers with payloads of size greater than or equal to this value will use the IPC capability. Transfers with smaller payloads will use CPU-attached shared memory regions.

           Default: 8192

       MPICH_GPU_NO_ASYNC_MEMCPY
           This variable toggles an optimization for intra-node MPI transfers involving CPU and GPU buffers. This optimization is enabled by default. If set to 1, it reverts to using blocking memcpy operations for
           intra-node MPI transfers involving CPU and GPU buffers. Depending on the GPU hardware being used, disabling this optimziation negatively affects the performance of large message intra-node MPI operations
           involving the CPU-to-GPU data paths.

           Default: 0

       MPICH_GPU_COLL_STAGING_AREA_OPT
           This variable toggles an optimization for certain collective operations involving GPU buffers. The optimization is currently implemented for MPI_Allreduce operations involving large payloads. This optimization
           is applicable for GPU-GPU transfers involving communication peers that are on the same compute node, or on different compute nodes. This is an experimental feature and is currently disabled by default. If set to
           1, this optimization is enabled.

           Default: 1

       MPICH_MEMORY_REPORT
           If set to 1, print a summary of the min/max high water mark and associated rank to stderr.

           If set to 2, output each rank’s high water mark to a file as specified using MPICH_MEMORY_REPORT_FILE.

           If set to 3, do both 1 and 2.

           The detailed report for each rank may be slightly higher than the summary because the summary is collected earlier during finalize and requires MPI collective calls, which may allocate more memory.

           Example #1: MPICH_MEMORY_REPORT=1

           This summary reports maximum and minimum values and the lowest rank that reported the value (max_loc/min_loc reductions). The by malloc lines are for malloc/free calls. The by mmap lines are for mmap/munmap
           calls. The by shmget lines are for shmget or SYSCALL(shmget) and shmctl(..RM_ID..)  calls.

                   # MPICH_MEMORY: Max memory allocated by malloc:  3898224 bytes by rank 40
                   # MPICH_MEMORY: Min memory allocated by malloc:  2805600 bytes by rank 61
                   # MPICH_MEMORY: Max memory allocated by mmap:    10485760 bytes by rank 0
                   # MPICH_MEMORY: Min memory allocated by mmap:    10485760 bytes by rank 0
                   # MPICH_MEMORY: Max memory allocated by shmget:  108821784 bytes by rank 0
                   # MPICH_MEMORY: Min memory allocated by shmget:  0 bytes by rank 1
                   # MPICH_MEMORY: Max memory reserved by symmetric heap:  2097152 bytes by rank 0
                   # MPICH_MEMORY: Min memory reserved by symmetric heap:  2097152 bytes by rank 0
                   # MPICH_MEMORY: Max memory allocated by symmetric heap: 1048576 bytes by rank 0
                   # MPICH_MEMORY: Min memory allocated by symmetric heap: 1048576 bytes by rank 0

           Example #2: MPICH_MEMORY_REPORT=2

           Each rank reports similar high water information to file.rank. Each line in the report begins with rank. This example is for rank 1.

               # [1] Max memory allocated by malloc:    2898408 bytes
               # [1] Max memory allocated by mmap:      10485760 bytes
               # [1] Max memory allocated by shmget:    0 bytes
               # [1] Max memory reserved by symmetric heap:  2097152 bytes
               # [1] Max memory allocated by symmetric heap: 1048576 bytes

           Default: not set (off)

       MPICH_MEMORY_REPORT_FILE
           Specifies the target path/prefix for the detailed high water mark list generated if MPICH_MEMORY_REPORT is set to 2 or 3. The actual filename for each high water mark report is this path/prefix plus the MPI rank
           number. If the specified target file cannot be opened, stderr is used.

           Default: stderr

       MPICH_NO_BUFFER_ALIAS_CHECK
           If enabled, the buffer alias error check for collectives is disabled. The MPI standard does not allow aliasing of type OUT or INOUT parameters on the same collective function call. The use of MPI_IN_PLACE is
           required in these scenarios. A check is in place to detect this condition and report the error. To bypass this check, set MPICH_NO_BUFFER_ALIAS_CHECK to any value.

           Default: Not enabled.

       MPICH_OPT_THREAD_SYNC
           Controls the mechanism used to implement thread-synchronization inside the MPI library. If set to 1, an optimized synchronization implementation is used. If set to 0, the library falls back to using the pthread
           mutex based thread-synchronization implementation. This variable is applicable only if the MPI_THREAD_MULTIPLE threading level is requested by the application during MPI initialization.

           Default: 1

       MPICH_OPTIMIZED_MEMCPY
           Specifies which version of memcpy to use. Valid values are:

           0
               Use the system (glibc) version of memcpy.

           1
               Use an optimized version of memcpy if one is available for the processor being used.

           2
               Use a highly optimized version of memcpy that provides better performance in some areas but may have performance regressions in other areas, if one is available for the processor being used.

               Default: 1

       MPICH_RANK_REORDER_DISPLAY
           If enabled, causes rank 0 to display which node each MPI rank resides in. The rank order can be manipulated via the MPICH_RANK_REORDER_METHOD environment variable.

           Default: Not enabled.

       MPICH_RANK_REORDER_FILE
           If MPICH_RANK_REORDER_METHOD is set to 3 and this variable is set, then the value of this variable is the file name that MPI will check for rank reordering information. If this variable is not set, then MPI will
           check the default file name, MPICH_RANK_ORDER. Default: Not set

       MPICH_RANK_REORDER_METHOD
           Overrides the default MPI rank placement scheme. To display the MPI rank placement information, enable MPICH_RANK_REORDER_DISPLAY.

           MPICH_RANK_REORDER_METHOD accepts the following values:

           0
               Specifies round-robin placement. Sequential MPI ranks are placed on the next node in the list. When every node has been used, the rank placement starts over again with the first node.

               For example, an 8-process job launched on 4 dual-core nodes would be placed as

                        NODE   RANK
                          0    0&4
                          1    1&5
                          2    2&6
                          3    3&7

               For example, an 8-process job launched on 2 quad-core nodes would be placed as

                        NODE   RANK
                          0    0&2&4&6
                          1    1&3&5&7

               A 24-process job launched on three 8-core nodes would be placed as

                        NODE   RANK
                          0    0&3&6&9&12&15&18&21
                          1    1&4&7&10&13&16&19&22
                          2    2&5&8&11&14&17&20&23

                   If the last node is not fully populated with MPI ranks using the default placement, no additional ranks can be placed on that node. A 20-process job launched on three 8-core nodes with round-robin
                   placement would be placed as:::

                        NODE   RANK
                          0    0&3&6&9&12&14&16&18
                          1    1&4&7&10&13&15&17&19
                          2    2&5&8&11

           1
               Specifies SMP-style placement. For a multi-core node, sequential MPI ranks are placed on the same node.

               For example, an 8-process job launched on 4 dual-core nodes would be placed as:::

                    NODE   RANK
                      0    0&1
                      1    2&3
                      2    4&5
                      3    6&7

               An 8-process job launched on 2 quad-core nodes would be placed as

                        NODE   RANK
                          0    0&1&2&3
                          1    4&5&6&7

               A 24-process job launched on three 8-core nodes would be placed as

                        NODE   RANK
                          0    0&1&2&3&4&5&6&7
                          1    8&9&10&11&12&13&14&15
                          2    16&17&18&19&20&21&22&23

           2
               Specifies folded-rank placement. Sequential MPI ranks are placed on the next node in the list. When every node has been used, instead of starting over with the first node again, the rank placement starts at
               the last node, going back to the first. For quad-core or larger nodes, this fold is repeated.

               For example, an 8-process job on 4 dual-core nodes would be placed as

                        NODE   RANK
                          0    0&7
                          1    1&6
                          2    2&5
                          3    3&4

               An 8-process job on 2 quad-core nodes would be placed as

                        NODE   RANK
                          0    0&3&4&7
                          1    1&2&5&6

               A 24-process job launched on three 8-core nodes would be placed as

                        NODE   RANK
                          0    0&5&6&11&12&17&18&23
                          1    1&4&7&10&13&16&19&22
                          2    2&3&8&9&14&15&20&21

           3
               Specifies a custom rank placement defined in the file named MPICH_RANK_ORDER. The MPICH_RANK_ORDER file must be readable by the first rank of the program, and reside in the current running directory. The
               order in which the ranks are listed in the file determines which ranks are placed closest to each other, starting with the first node in the list. To help with creating this file, consider using the
               grid_order tool from the Perftools package.

               The PALS launcher forwards stdin to original rank 0 in MPI_COMM_WORLD only. If your application requires stdin to be available to rank 0, any rank reorder arrangement must not reorder the original rank 0. If
               your application does not read from stdin, rank 0 can be reordered.

               For example:

               0-15
                   Places the ranks in SMP-style order (see above).

               15-0
                   For dual-core processors, places ranks 15&14 on the first node, ranks 13&12 on the next node, and so on. For quad-core processors, places ranks 15&14&13&12 on the first node, ranks 11&10&9&8 on the next
                   node, and so on.

               4,1,5,2,6,3,7,0,...
                   Places the first n ranks listed on the first node, the next n ranks on the next node, and so on, where n is the number of processes launched on each node.

                   You can use combinations of ranges (8-15) or individual rank numbers in the MPICH_RANK_ORDER file. The number of ranks listed in this file must match the number of processes launched.

                   A # denotes the beginning of a comment. A comment can start in the middle of a line and will continue to the end of the line.

       MPICH_RMA_MAX_PENDING
           Determines how many RMA network operations may be outstanding at any time. RMA operations beyond this max will be queued and only issued as pending operations complete.

           Default: 64

       MPICH_RMA_SHM_ACCUMULATE
           If set to 1, enables accumulate operations using shm shared memory. If set to 0, disables shm and accumulates will use other implementations. It also sets the default for the window hint "disable_shm_accumulate"
           to true if MPICH_RMA_SHM_ACCUMULATE is 0, and false if MPICH_RMA_SHM_ACCUMULATE is 1.

           Default: 1

       MPICH_SINGLE_HOST_ENABLED
           If enabled, prevents MPICH from using networking hardware when all ranks are on a single host. This avoids the unnecessary consumption of networking resources. This feature is only usable when MPI Spawn is not
           possible due to all possible ranks in MPI_UNIVERSE_SIZE already being part of MPI_COMM_WORLD.

           Default: Enabled

       MPICH_VERSION_DISPLAY
           If enabled, causes MPICH to display the HPE Cray MPI version number as well as build date information. The version number can also be accessed through the attribute CRAY_MPICH_VERSION.

           Default: Not enabled.

   SMP ENVIRONMENT VARIABLES
       MPICH_SHM_PROGRESS_MAX_BATCH_SIZE
           Adjusts the maximum number of on-node requests that can be processed in a single batch. Higher values for the maximum batch size can lower the overhead due to entering the progress engine, but can also delay the
           processing of off-node message requests.

           Default: 8

       MPICH_SMP_SINGLE_COPY_MODE
           If set, selects the on-node implementation for large messages. This variable can be set to XPMEM, CMA or NONE. By default, Cray MPICH will attempt to use the single-copy-based implementation via XPMEM. If XPMEM
           is not available, Cray MPICH will fallback to using CMA. If CMA is also not available, CRAY MPICH will fallback to the two-copy-based shared-memory implementation. If this variable is set to NONE, it overrides
           the MPICH_SMP_SINGLE_COPY_SIZE setting.

           Default: XPMEM

       MPICH_SMP_SINGLE_COPY_SIZE
           Specifies the minimum message size in bytes to consider for single-copy transfers for on-node messages. This applies only to the SMP (on-node shared memory) device. The value is interpreted as bytes, unless the
           string ends in a K or M, which indicates kilobytes or megabytes, respectively.

           Valid values are between 512 and approximately 65536 bytes.

           Default: 8192 bytes

   LIBFABRIC ENVIRONMENT VARIABLES
       FI_OFI_RXM_BUFFER_SIZE
           This is a verbs;ofi_rxm libfabric ENV variable. It specifies the transmit buffer size/inject size in bytes. Messages of size less than this will be transmitted via an eager protocol and those above will be
           transmitted via a rendezvous or SAR (Segmentation And Reassembly) protocol. Note that user data is copied up to this size.

           Default: 16364

       FI_OFI_RXM_SAR_LIMIT
           This is a verbs;ofi_rxm libfabric ENV variable. Set this environment variable to control the SAR (Segmentation And Reassembly) protocol. The SAR protocol breaks a message into smaller units before transmission
           and reassembles them into the proper order at the receiving end. Messages of size greater than this (in bytes) are transmitted via rendezvous protocol. Setting this to 0 disables SAR protocol entirely. When set
           to 0, messages will be transferred by either the eager or rendezvous protocols.

           Default: 262144

       FI_OFI_RXM_RX_SIZE
           This is a verbs;ofi_rxm libfabric ENV variable. Adjust this environment variable to control the size of the receive queue. The size of a receive queue dictates how many outstanding receives the application can
           post at a time, without processing any matching sends. In some applications, the number of outstanding receives might exceed the size of the receive queue and cause the application to deadlock if no matching
           sends are received. Increasing the queue size can mitigate the problem. Note the default queue size should satisfy most applications, and libfabric will consume more memory with a larger receive queue size.

           Default: 4096

       FI_OFI_RXM_USE_SRX
           This is a verbs;ofi_rxm libfabric ENV variable. Set this to 1 to instruct the provider to use shared receive queues. Using shared receive queues can reduce the overall memory usage significantly, but may cause
           latency to increase slightly.

           Default behavior if unset:

                For jobs sizes of < 64 ranks, default is 0
                For job sizes of 64 ranks or larger, default is 1

       FI_VERBS_PREFER_XRC
           This is a verbs;ofi_rxm libfabric ENV variable. Set this to 1 to request use of the XRC (eXtended Reliable Connection) protocol. Note FI_OFI_RXM_USE_SRX must also be set to 1 when requesting XRC. Using the XRC
           protocol reduces the number of connections, hardware resources, and memory footprint for large scaling jobs that require a demanding communication pattern. This environment variable is required when scaling jobs
           with an all-to-all communication pattern.

           Default behavior if unset:

                For jobs sizes of < 64 ranks, default is 0
                For job sizes of 64 ranks or larger, default is 1

       FI_VERBS_MIN_RNR_TIMER
           This is a verbs;ofi_rxm libfabric ENV variable. This sets the minimum backoff time used when the Mellanox NICs experience congestion. Allowable values are 0-31, with higher values corresponding to longer
           backoffs. Setting this to 0 is not recommended, however, as that translates into a very large backoff and will adversely affect performance. Optimal value for Slingshot-10 systems are likely between 3 and 6.

           Default: 6

       MPICH_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS
           If set to 1, MPICH will request asynchronous automatic control progress (FI_PROGRESS_AUTO) from the libfabrics provider. This may require an additional libfabrics thread to make asynchronous progress on all
           communication. This option is highly recommended for applications using RMA passive communication. Not that it might cause a small amount of performance overhead for regular MPI operations. The user is
           encouraged to leave one or more hardware threads vacant in order to prevent contention between the application threads and the progress thread(s). The impact of oversubscription is highly system dependent but
           may be substantial in some cases, hence this recommendation.

           Default: not set

       MPICH_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS
           If set to 1, MPICH will request asynchronous automatic data progress (FI_PROGRESS_AUTO) from the libfabrics provider. This may require an additional libfabrics thread to make asynchronous progress on all
           communication. This option is highly recommended for applications using RMA passive communication. Not that it might cause a small amount of performance overhead for regular MPI operations. The user is
           encouraged to leave one or more hardware threads vacant in order to prevent contention between the application threads and the progress thread(s). The impact of oversubscription is highly system dependent but
           may be substantial in some cases, hence this recommendation.

           Default: not set

       MPICH_OFI_NIC_MAPPING
           Specifies the precise rank-to-NIC mapping to use on each node. This is evaluated only if the MPICH_OFI_NIC_POLICY variable is set to USER. This mapping is based on the zero-based local rank value, not global
           rank value. Each local rank must have a NIC mapping assigned by this variable. If there are fewer MPI ranks on any node, that portion of the MPICH_OFI_NIC_MAPPING string will be ignored. Add quotes around the
           entire string to prevent the shell from interpreting the value incorrectly.

           The format is as follows
               "nic_idx:local_mpi_ranks; nic_idx:local_mpi_ranks; nic_idx;local_mpi_ranks"

               Examples assume 64 ranks placed per node, with each node having 2 or 3 NICs.

                   To assign local_rank 0 to NIC 0, and remaining ranks to NIC 1, use:

                    MPICH_OFI_NIC_MAPPING="0:0; 1:1-63"

                   To assign local ranks 0,16,32,48 to NIC 0, and remaining ranks to NIC 1:

                    MPICH_OFI_NIC_MAPPING="0:0,16,32,48; 1:1-15,17-31,33-47,49-63"

                   To assign local ranks 0-7 to NIC 0, 8-31 to NIC 2, and 32-63 to NIC 1:

                    MPICH_OFI_NIC_MAPPING="0:0-7; 2:8-31; 1:32-63"

       Default: not set

       MPICH_OFI_NIC_POLICY
           Selects the rank-to-NIC assignment policy used by Cray MPI. Each MPI rank will be assigned to exactly one NIC. There are four available options: [BLOCK | ROUND-ROBIN | NUMA | GPU | USER].

           BLOCK
               Selects a block distribution. Consecutive local ranks on a node are equally distributed among the available NICs on the node. The number of ranks on a node are divided by the number of NICs on that node
               (rounded up), with the first X local ranks assigned to NIC 0, the next X local ranks assigned to NIC 1, etc.

               For example, with 22 ranks placed per node, and each node having 4 NICs

                         ranks 0-5 are assigned to NIC 0
                         ranks 6-11 are assigned to NIC 1
                         ranks 12-17 are assigned to NIC 2
                         ranks 18-21 are assigned to NIC 3

           ROUND-ROBIN
               Selects a round-robin distribution. The first local rank on a node is assigned to NIC 0, the second rank is assigned NIC 1, the third rank is assigned NIC 2, etc. When all NICs on the node have been assigned
               once, the next available local rank will be assigned NIC 0, and so on.

               For example, with 22 ranks placed per node, and each node having 4 NICs

                         ranks 0,4,8,12,16,20 are assigned to NIC 0
                         ranks 1,5,9,13,17,21 are assigned to NIC 1
                         ranks 2,6,10,14,18 are assigned to NIC 2
                         ranks 3,7,11,15,19 are assigned to NIC 3

           NUMA
               Selects a NUMA-aware distribution. The local ranks are assigned to the NIC that is closest to the rank’s numa node affinity. If a rank is pinned to a core or subset of cores in numa node N, and a NIC is also
               mapped to numa node N, the rank will use that corresponding NIC. If a matching numa node between rank and NIC is not found, then the NIC in the closest numa node to the rank is selected. If multiple NICs are
               assigned to the same numa node, the local ranks will round-robin between them. Numa distances are analyzed to select the closest NIC.

               For the NUMA policy to be successful when multiple NICs per node are available, the affinity of the ranks must be constrained (pinned) to cores contained within a single numa node. A rank is not allowed to
               float among cores that span numa nodes when selecting the NUMA policy. If that condition exists, the job will abort with an error message.

           GPU
               Selects a GPU-aware distribution. The local ranks are assigned to the NIC that is closest to the GPU selected by the user via a vendor API (e.g. CUDA or HIP). If multiple NICs are assigned to the same
               GPU/numa node, the local ranks will round-robin between them. Numa distances are analyzed to select the NIC closest to the user’s selected GPU.

           USER
               Supports a custom user-selection for NIC assignment. This selection requires the MPICH_OFI_NIC_MAPPING variable to also be set to indicate the precise rank-to-NIC assignment requested. See
               MPICH_OFI_NIC_MAPPING.

       Default: BLOCK

       MPICH_OFI_NIC_VERBOSE
           If set to 1, verbose information pertaining to NIC selection is printed at the start of the job. All available NIC domain names, addresses and index values are displayed. Setting this variable to 2 displays
           additional details, including the specific NIC each rank has been assigned, which is based on MPICH_OFI_NIC_POLICY.

           Default: 0

       MPICH_OFI_NUM_NICS
           Specifies the number of NICs the job can use on a per-node basis. By default, when multiple NICs per node are available, MPI attempts to use them all. If fewer NICs are desired, this variable can be set to
           indicate the maximum number of NICs per node MPI will use. By default, MPI uses consecutive NIC indices, starting with index 0.

           To request MPI to use alternative NIC index values, an optional segment can be added to this variable by adding a colon followed by the desired nic index values. Add quotes around the entire string to prevent
           the shell from interpreting the value incorrectly.

           For example:

               To use 1 NIC per node, index 0, specify:
                  export MPICH_OFI_NUM_NICS=1   (equivalent to MPICH_OFI_NUM_NICS="1:0")

               To use 1 NIC per node, index 1, specify:
                   export MPICH_OFI_NUM_NICS="1:1"

               To use 2 NICs per node, index 0 and 1, specify:
                  export MPICH_OFI_NUM_NICS=2    (equivalent to MPICH_OFI_NUM_NICS="2:0,1")

               To use 2 NICs per node, index 1 and 3, specify
                  export MPICH_OFI_NUM_NICS="2:1,3"

           Default: not set (MPI uses all available NICs by default)

       MPICH_OFI_RMA_STARTUP_CONNECT
           By default, OFI connections between ranks are set up on demand. This allows for optimal performance while minimizing memory requirements. However, for RMA jobs requiring an all-to-all on-node communication
           pattern, it may be beneficial to create OFI connections between PEs on a node in a coordinated manner at startup. If set to 1, Cray MPI will create connections between all ranks on each node in the job during
           MPI_Init. This option also automatically enables MPICH_OFI_STARTUP_CONNECT.

           Default: 0

       MPICH_OFI_SKIP_NIC_SELECTION
           If set to 1, the Cray MPI NIC selection algorithm is bypassed. In this case, only the first NIC presented via the libfabric interface is used. None of the MPICH_OFI_NIC_xxx environment variables are processed if
           this variable is set. This mode is for debug purposes.

           Default: 0

       MPICH_OFI_SKIP_NIC_SYMMETRY_TEST
           If set to 1, the check for NIC symmetry performed during MPI_Init will be bypassed. By default, a symmetry check is run to make sure all the nodes in the job have the same number of NICs available. An asymmetric
           NIC layout can pose significant performance implications, especially if the user is unaware of this condition.

           Default: 0

       MPICH_OFI_STARTUP_CONNECT
           By default, OFI connections between ranks are set up on demand. This allows for optimal performance while minimizing memory requirements. However, for jobs requiring an all-to-all communication pattern, it may
           be beneficial to create all OFI connections in a coordinated manner at startup. If set to 1, Cray MPI will create connections between all ranks in the job during MPI_Init.

           Default: 0

       MPICH_OFI_USE_PROVIDER
           Specifies the libfabric provider to use. By default, the "verbs;ofi_rxm" provider is selected for Slingshot-10 systems, as this is the supported and optimized provider. For debugging purposes, other libfabric
           providers may be requested by setting this variable to the desired provider name (i.e. sockets or "tcp;ofi_rxm").

           Default: "verbs;ofi_rxm" on Slingshot-10 systems

       MPICH_OFI_VERBOSE
           If set, more verbose output will be displayed during MPI_Init to verify which libfabric provider has been selected, along with the name and address of the NIC being used. This may be helpful for debugging errors
           encountered during MPI_Init.

           Default: not set

   COLLECTIVE ENVIRONMENT VARIABLES
       MPICH_ALLGATHER_VSHORT_MSG
           Adjusts the cutoff point at and below which the architecture-specific optimized gather/bcast algorithm is used instead of the optimized ring algorithm for MPI_Allgather. The gather/bcast algorithm is better
           suited for small messages.

           Defaults:

                For communicator sizes of <= 512 ranks, 1024 bytes.
                For communicator sizes of > 512 ranks, 4096 bytes.

       MPICH_ALLGATHERV_VSHORT_MSG
           Adjusts the cutoff point at and below which the architecture-specific optimized gatherv/bcast algorithm is used instead of the optimized ring algorithm for MPI_Allgatherv. The gatherv/bcast algorithm is better
           suited for small messages.

           Defaults:

                For communicator sizes of <= 512 ranks, 1024 bytes.
                For communicator sizes of > 512 ranks, 4096 bytes.

       MPICH_ALLREDUCE_BLK_SIZE
           Specifies the block size (in bytes) to use when dividing very large Allreduce messages into smaller blocks for better performance. The value is interpreted as bytes, unless the string ends in a K, which
           indicates kilobytes, or M, which indicates megabytes. Valid values are between 8192 and MAX_INT.

           Default: 716800 bytes

       MPICH_ALLREDUCE_NO_SMP
           If set, MPI_Allreduce uses an algorithm that is not smp-aware. This provides a consistent ordering of the specified allreduce operation regardless of system configuration.

           Note: This algorithm may not perform as well as the default smp-aware algorithms as it does not take advantage of rank topology.

           Default: not set

       MPICH_ALLTOALL_SHORT_MSG
           Adjusts the cut-off points at and below which the store and forward Alltoall algorithm is used for short messages. The default value is dependent upon the total number of ranks in the MPI communicator used for
           the MPI_Alltoall call.

           Defaults:

                if communicator size <= 1024, 512 bytes
                if communicator size > 1024 and <= 65536, 256 bytes
                if communicator size > 65536 and <= 131072, 128 bytes
                if communicator size > 131072, 64 bytes

       MPICH_ALLTOALL_SYNC_FREQ
           Adjusts the number of outstanding messages (the synchronization frequency) each rank participating in the Alltoall algorithm will allow. The defaults vary for each call, depending on several factors, including
           number of ranks on a node participating in the collective, and the message size.

           Default: Varies from 1 to 24

       MPICH_ALLTOALLV_THROTTLE
           Sets the per-process maximum number of outstanding Isends and Irecvs that can be posted concurrently for the optimized send/recv MPI_Alltoallv algorithm. For sparsely-populated Alltoallv data, setting this to a
           higher value may improve performance. For heavily-populated large message Alltoallv data, consider decreasing this value.

           Default: 8

       MPICH_BCAST_INTERNODE_RADIX
           Used to set the radix of the inter-node tree. This can be set to any integer value greater than or equal to 2.

           Default: 4

       MPICH_BCAST_INTRANODE_RADIX
           Used to set the radix of the intra-node tree. This can be set to any integer value greater than or equal to 2.

           Default: 4

       MPICH_BCAST_ONLY_TREE
           If set to 1, MPI_Bcast uses an smp-aware tree algorithm regardless of data size. The tree algorithm generally scales well to high processor counts.

           If set to 0, MPI_Bcast uses a variety of algorithms (tree, scatter, or ring) depending on message size and other factors.

           Default: 1

       MPICH_COLL_OPT_OFF
           If set, disables collective optimizations which use nondefault, architecture-specific algorithms for some MPI collective operations. By default, all collective optimized algorithms are enabled.

           To disable all collective optimized algorithms, set MPICH_COLL_OPT_OFF to 1.

           To disable optimized algorithms for selected MPI collectives, set the value to a comma-separated list of the desired collective names. Names are not case-sensitive. Any unrecognizable name is flagged with a
           warning message and ignored. For example, to disable the MPI_Allgather optimized collective algorithm, set MPICH_COLL_OPT_OFF=mpi_allgather.

           The following collective names are recognized: MPI_Allgather, MPI_Allgatherv, MPI_Allreduce, MPI_Alltoall, MPI_Alltoallv, MPI_Bcast, MPI_Gatherv, MPI_Scatterv, MPI_Igatherv, and MPI_Iallreduce.

           Default: Not enabled.

       MPICH_GATHERV_MAX_TMP_SIZE
           Only applicable to the Gatherv tree algorithm. Sets the maximum amount of temporary memory Gatherv will allow a rank to allocate when using the tree-based algorithm. Each rank allocates a different amount, with
           many allocating no extra memory. If any rank requires more than this amount of temporary buffer space, a different algorithm is used.

           Default: 512M

       MPICH_GATHERV_MIN_COMM_SIZE
           Cray MPI offers two optimized Gatherv algorithms: a tree algorithm for small messages and a permission-to-send algorithm for larger messages. Set this value to the minimum communicator size to attempt use of
           either of the Cray optimized Gatherv algorithms. Smaller communicator sizes will use the ANL MPI_Gatherv algorithm.

           Default: 64

       MPICH_GATHERV_SHORT_MSG
           Adjusts the cutoff point at and below which the optimized tree MPI_Gatherv algorithm is used instead of the optimized permission-to-send algorithm. The cutoff is in bytes, based on the average size of the
           variable MPI_Gatherv message sizes.

           Default: 131072

       MPICH_GATHERV_SYNC_FREQ
           Only applicable to the Gatherv permission-to-send algorithm. Adjusts the number of outstanding receives the root for Gatherv will allow.

           Default: 16

       MPICH_REDUCE_NO_SMP
           If set, MPI_Reduce uses an algorithm that is not smp-aware. This provides a consistent ordering of the specified reduce operation regardless of system configuration.

           Note: This algorithm may not perform as well as the default smp-aware algorithms as it does not take advantage of rank topology.

           Default: not set

       MPICH_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE
           This environment variable applies to MPI_Reduce_scatter and MPI_Reduce_scatter_block. For the reduce_scatter functions, this variable specifies the cutoff size of the send buffer (in bytes) at and above which a
           pairwise exchange algorithm is attempted. In addition, the op must be commutative and the communicator size less than or equal to MPICH_REDUCE_SCATTER_MAX_COMMSIZE for the pairwise exchange algorithm to be used.
           For smaller send buffers, a recursive halving algorithm is used.

           Default value: 524288

       MPICH_REDUCE_SCATTER_MAX_COMMSIZE
           This environment variable applies to MPI_Reduce_scatter and MPI_Reduce_scatter_block. For the reduce_scatter functions, this variable specifies the maximum communicator size that triggers use of the pairwise
           exchange algorithm, provided the op is commutative. The pairwise exchange algorithm is not well-suited for scaling to high process counts, so for larger communicators, a recursive halving algorithm is used by
           default instead.

           Default value: 1000

       MPICH_SCATTERV_MAX_TMP_SIZE
           Only applicable to the Scatterv tree algorithm. Sets the maximum amount of temporary memory Scatterv will allow a rank to allocate when using the tree-based algorithm. Each rank allocates a different amount,
           with many allocating no extra memory. If any rank requires more than this amount of temporary buffer space, a different algorithm is used.

           Default: 512M

       MPICH_SCATTERV_MIN_COMM_SIZE
           Cray MPI offers two optimized Scatterv algorithms: a tree algorithm for small messages and a staggered send algorithm for larger messages. Set this value to the minimum communicator size to attempt use of either
           of the Cray optimized Scatterv algorithms. Smaller communicator sizes will use the ANL MPI_Scatterv algorithm.

           Default: 64

       MPICH_SCATTERV_SHORT_MSG
           Adjusts the cutoff point at and below which the optimized tree MPI_Scatterv algorithm is used instead of the optimized staggered send algorithm. The cutoff is in bytes, based on the average size of the variable
           MPI_Scatterv message sizes.

           Default behavior if unset is:

           For communicator sizes of < or = 512 ranks, 2048 bytes

           For communicator sizes of > 512 ranks, 8192 bytes

       MPICH_SCATTERV_SYNCHRONOUS
           Only applicable to the ANL non-optimized Scatterv algorithm. The ANL MPI_Scatterv algorithm uses asynchronous sends for communicator sizes less than 200,000 ranks. If set, this environment variable causes the
           ANL MPI_Scatterv algorithm to switch to using blocking sends, which may be beneficial with large data sizes or high process counts.

           For communicator sizes equal to or greater than 200,000 ranks, the blocking send algorithm is used by default.

           Default: not enabled

       MPICH_SCATTERV_SYNC_FREQ
           Only applicable to the Scatterv staggered send algorithm. Adjusts the number of outstanding sends the root for Scatterv will use.

           Default: 16

       MPICH_SHARED_MEM_COLL_OPT
           By default, the MPICH library will use the optimized shared-memory based design for collective operations. The supported collective operations are: MPI_Allreduce, MPI_Barrier, and MPI_Bcast.

           To disable all available shared-memory optimizations, set MPICH_SHARED_MEM_COLL_OPT to 0.

           To enable this feature for a specific set of collective operations, set MPICH_SHARED_MEM_COLL_OPT to a comma-separated list of collective names. For example, to enable this optimization for MPI_Bcast only, set
           MPICH_SHARED_MEM_COLL_OPT=MPI_Bcast. To enable this optimization for MPI_Allreduce only, set MPICH_SHARED_MEM_COLL_OPT=MPI_Allreduce. Unsupported names are flagged with a warning message and ignored.

           Default: set

   MPI-IO ENVIRONMENT VARIABLES
       MPICH_MPIIO_AGGREGATOR_PLACEMENT_DISPLAY
           If enabled, displays the assignment of MPIIO collective buffering aggregators for reads/writes of a shared file, showing rank and node ID (nid). For example:

           Aggregator Placement for /lus/scratch/myfile
           RankReorderMethod=3  AggPlacementStride=-1
            AGG    Rank       nid
            ----  ------  --------
               0       0  nid00578
               1       4  nid00579
               2       1  nid00606
               3       5  nid00607
               4       2  nid00578
               5       6  nid00579
               6       3  nid00606
               7       7  nid00607

           Default: Not enabled.

       MPICH_MPIIO_AGGREGATOR_PLACEMENT_STRIDE
           Partially controls to which nodes MPIIO collective buffering aggregators are assigned. See the notes below on the order of nodes. Network traffic and resulting I/O performance may be affected by the assignments.

           If set to 1, consecutive nodes are used. The number of aggregators assigned per node is controlled by the cb_config_list hint. By default, no more than one aggregator per node will be assigned if there are at
           least as many nodes as aggregators.

           If set to a value greater than 1, node selection is strided across the available nodes by this value. If the stride times the number of aggregators exceeds the number of nodes, the assignments will wrap around,
           which is usually not optimal for performance.

           If set to -1, node selection is strided across available nodes by the value of the number of nodes divided by the number of aggregators (integer division, minimum value of 1). The purpose is to spread out the
           nodes to reduce network congestion.

           Note: The order of nodes can be shown by setting the MPICH_RANK_REORDER_DISPLAY environment variable. This lists in rank order (rank for MPI_COMM_WORLD) the node on which each rank resides. When there is more
           than one rank per node, the node ID is repeated. When MPI has not done any rank reordering, all the ranks for the first node are listed first, then all the ranks for the second node are listed second, etc. When
           rank reordering has been done, see the MPICH_RANK_REORDER_METHOD environment variable, the order of the nodes can be very different. To spread the aggregators across the nodes, if MPICH_RANK_REORDER_METHOD=3,
           MPIIO sorts the list by nid and then by rank on that node, for the node order used when assigning aggregators. This has the desired effect of spreading the aggregators across the nodes assigned to the job.The
           current implementation is not file-specific. That is, the environment variable applies to all files opened with MPI_File_open().

           Default: -1

       MPICH_MPIIO_CB_ALIGN
           Sets the default value for the cb_align hint. Files opened with MPI_File_open wil have this value for the cb_align hint unless the hint is set on a per file basis with either the MPICH_MPIIO_HINTS environment
           variable or from within a program with the MPI_Info_set() call.

           Note: Only MPICH_MPIIO_CB_ALIGN == 2 is fully supported. Other values are for internal testing only.

           Default: 2

       MPICH_MPIIO_DVS_MAXNODES
           Note: This environment variable in relevant only for file systems accessed from HPE system compute nodes via DVS server nodes; e.g. GPFS or PANFS.

           As described in the dvs(5) man page, the environment variable DVS_MAXNODES can be used to set the stripe width—that is, the number of DVS server nodes—used to access a file in "stripe parallel mode." For most
           files, and especially for small files, setting DVS_MAXNODES to 1 ("cluster parallel mode") is preferred.

           The MPICH_MPIIO_DVS_MAXNODES environment variable enables you to leave DVS_MAXNODES set to 1 and then use MPICH_MPIIO_DVS_MAXNODES to temporarily override DVS_MAXNODES when it is advantageous to specify wider
           striping for files being opened by the MPI_File_open() call. The range of values accepted by MPICH_MPIIO_DVS_MAXNODES goes from 1 to the number of server nodes specified on the mount with the nnodes mount
           option.

           DVS_MAXNODES is not set by default. Therefore, for MPICH_MPIIO_DVS_MAXNODES to have any effect, DVS_MAXNODES must be defined before program startup and defined using exactly three characters, where the
           characters specify the decimal value and the remainder are underscore characters: for example, DVS_MAXNODES=12_. If DVS_MAXNODES is not defined or defined incorrectly, MPI-IO will ignore
           MPICH_MPIIO_DVS_MAXNODES. A warning message is issued if the value requested by the user does not match the value actually used by DVS.

           MPICH_MPIIO_DVS_MAXNODES interacts with MPICH_MPIIO_HINTS. To determine the striping actually used, the order of precedence is:

           striping_factor set using MPICH_MPIIO_HINTS, if set

           striping_factor set using MPI_Info_set(), if set

           MPICH_MPIIO_DVS_MAXNODES value, if set

           DVS_MAXNODES, if set

           DVS maxnodes=n mount option, if specified

           Default: unset

       MPICH_MPIIO_HINTS
           If set, override the default value of one or more MPI I/O hints. This also overrides any values that were set by using calls to MPI_Info_set in the application code. The new values apply to the file the next
           time it is opened using an MPI_File_open() call.

           After the MPI_File_open() call, subsequent MPI_Info_set calls can be used to pass new MPI I/O hints that take precedence over some of the environment variable values. Other MPI I/O hints such as striping_factor,
           striping_unit, cb_nodes, and cb_config_list cannot be changed after the MPI_File_open() call, as these are evaluated and applied only during the file open process.

           An MPI_File_close call followed by an MPI_File_open call can be used to restart the MPI I/O hint evaluation process.

           The syntax for this environment variable is a comma-separated list of specifications. Each individual specification is a pathname_pattern followed by a colon-separated list of one or more key=value pairs. In
           each key=value pair, the key is the MPI-IO hint name, and the value is its value as it would be coded for an MPI_Info_set library call.

           For example:

           MPICH_MPIIO_HINTS=spec1[,spec2,...]

           Where each specification has the syntax:

           pathname_pattern:key1=value1[:key2=value2:...]

           The pathname_pattern can be an exact match with the filename argument used in the MPI_File_open() call or it can be a pattern as described below.

           When a file is opened with MPI_File_open(), the list of hint specifications in the MPICH_MPIIO_HINTS environment variable is scanned. The first pathname_pattern matching the filename argument in the
           MPI_File_open() call is selected. Any hints associated with the selected pathname_pattern are applied to the file being opened. If no pattern matches, no hints from this specification are applied to the file.

           The pathname_pattern follows standard shell pattern-matching rules with these meta-characters:

           ----------------------------------------------------------------
           Pattern    Description
           ----------------------------------------------------------------
           *          Match any number of characters
           ?          Match any single character
           [a-b]      Match any single character between a and b, inclusive
           \          Interpret the meta-character that follows literally
           ----------------------------------------------------------------

           The simplest pathname_pattern is . Using this results in the specified hints being applied to all files opened with *MPI_File_open(). Use of this wildcard is discouraged because of the possibility that a library
           linked with the application may also open a file for which the hints are not appropriate. * The following example shows how to set hints for a set of files. The final specification in this example, for file
           /scratch/user/me/dump., has two *key=value pairs.

           MPICH_MPIIO_HINTS=file1:direct_io=true,file2:romio_ds_write=disable,\
           /scratch/user/me/dump.*:romio_cb_write=enable:cb_nodes=8

           The following MPI-IO key values are supported on HPE systems.

           abort_on_rw_error
               If set to enable, causes MPI-IO to abort immediately after issuing an error message if an I/O error occurs during a system read() or system write() call. The valid values are enable and disable. See the
               MPICH_MPIIO_ABORT_ON_RW_ERROR environment variable for more details.

               Default: disable

           cb_align
               Specifies which alignment algorithm to use for collective buffering. If set to 2, an algorithm is used to divide the I/O workload into Lustre stripe-sized pieces and assigns these pieces to collective
               buffering nodes (aggregators) so that each aggregator always accesses the same set of stripes and no other aggregator accesses those stripes. This is generally the optimal collective buffering mode as it
               minimizes the Lustre file system extent lock contention and thus reduces system I/O time.

               Historically there have been a few different collective buffering alignment algorithms used on HPE systems. Currently only one of them, algorithm 2, is supported. The alignment value of 1 is no longer
               supported. The alignment values of 0 and 3 are not fully supported but are for internal testing only. Other algorithms may be supported in the future.

               Default: 2

           cb_buffer_size
               Sets the buffer size in bytes for collective buffering.

               This hint is not used with the current default collective buffering algorithm.

           cb_config_list
               Specifies by name which nodes are to serve as aggregators. The syntax for the value is:

                   #name1:maxprocesses[,name2:maxprocesses,...]#

               Where name is either * (match all node names) or the name returned by MPI_Get_processor_name, and maxprocesses specifies the maximum number of processes on that node to serve as aggregators. If the value of
               the cb_nodes hint is greater than the number of compute nodes, the value of maxprocesses must be greater than 1 in order to assign the required number of aggregators. When the cb_align hint is set to 2 (the
               default), the aggregators are assigned using a round-robin method across compute nodes.

               The pair of # characters beginning and ending the list are not part of the normal MPIIO hint syntax but are required. Because colon (:) characters are used in both this list and in the MPICH_MPIIO_HINTS
               environment variable syntax, the # characters are required in order to determine the meaning of colon (:) character.

               This value cannot be changed after the file is opened.

               Default: :

           cb_nodes
               Specifies the number of aggregators used to perform the physical I/O for collective I/O operations when collective buffering is enabled. On multi-core nodes, all cores share the same node name.

               With the current default collective buffering algorithm, the best value for cb_nodes is usually the same as striping_factor (in other words, the stripe count).

               Default: striping_factor

           cray_cb_nodes_multiplier
               Specifies the number of collective buffering aggregators (cb_nodes) per OST for Lustre files. In other words, the number of aggregators is the stripe count (striping_factor) times the multiplier. This may
               improve or degrade I/O performance, depending on the file locking mode and other conditions. When the locking mode is 0, a multiplier of 1 is usually best for writing the file. When the locking mode is 1 or
               2, a multiplier of 2 or more is usually best for writing the file. If a locking mode is specified and both cb_nodes and cray_cb_nodes_multiplier hints are set, the cb_nodes hint is ignored. See
               cray_cb_write_lock_mode.

               When reading a file with collective buffering, a multiplier of 2 or more often improves read performance.

               Note: If the number of aggregators exceeds the number of compute nodes, performance generally won’t improve over 1 aggregator per compute node.

               Default: 1

           cray_cb_write_lock_mode
               Specifies the file locking mode for accessing Lustre files. These modes do not apply when accessing other file systems. Valid values are:

               0
                   Standard locking mode. Extent locks are held by each MPI rank accessing the file. The extent of each lock often exceeds the byte range needed by the rank. Locks are revoked and reissued when the extent
                   of a lock held by one rank conflicts with the extent of a lock needed by another rank.

               1
                   Shared lock locking mode. A single lock is shared by all MPI ranks that are writing the file. This lock mode is only applicable when collective buffering is enabled and is only valid if the only accesses
                   to the file are writes and all the writes are done by the collective buffering aggregators. The romio_no_indep_rw hint must be set to true to use this locking mode. This is an explicit assertion that all
                   file accesses will be with MPI collective I/O. Setting the romio_no_indep_rw hint to true also sets romio_cb_write and romio_cb_read to enable. Any other MPI I/O accesses will cause the program to abort
                   and any non-MPI I/O access may cause the program to hang. Both HDF5 and netCDF do both collective and independent I/O so this locking mode is not appropriate for these APIs.

                   Locking modes 1 and 2 reduce lock contention between multiple clients and therefore support greater parallelism by allowing multiple aggregators per OST to efficiently write to the file. Set the
                   cray_cb_nodes_multiplier hint to 2 or more to get the increased parallelism. The optimal value depends on file system characteristics. Note that if lock mode 2 is not supported, a warning will be printed
                   and the lock mode will be reset to 0.

                   Default: 0

           direct_io
               Enables the O_DIRECT mode for the specified file. The user is responsible for aligning the write or read buffer on a getpagesize() boundary. MPI-IO checks for alignment and aborts if it is not aligned. Valid
               values are true or false.

               Default: false.

           ind_rd_buffer_size
               Specifies in bytes the size of the buffer to be used for data sieving on read.

               Default: 4194304

           ind_wr_buffer_size
               Specifies in bytes the size of the buffer to be used for data sieving on write.

               Default: 524288

           romio_cb_read
               Enables collective buffering on read when collective IO operations are used. Valid values are enable, disable, and automatic. In automatic mode, whether or not collective buffering is done is based on
               runtime heuristics. When MPICH_MPIIO_CB_ALIGN is set to 2, the heuristics favor collective buffering.

               Default: automatic.

           romio_cb_write
               Enables collective buffering on write when collective IO operations are used. Valid values are enable, disable, and automatic. In automatic mode, whether or not collective buffering is done is based on
               runtime heuristics. When MPICH_MPIIO_CB_ALIGN is set to 2, the heuristics favor collective buffering.

               Default: automatic.

           romio_ds_read
               Specifies if data sieving is to be done on read. Valid values are enable, disable, and automatic.

               Default: disable

           romio_ds_write
               Specifies if data sieving is to be done on write. Valid values are enable, disable, and automatic. When set to automatic, data sieving on write is turned off if the MPI library has been initialized with
               MPI_THREAD_MULTIPLE. Setting the value enable will turn on data sieving on write irrespective of the thread environment and is safe as long as MPI-IO routines aren’t called concurrently from threads in a
               rank. Additionally, in order to avoid data corruption, it is necesary that data sieving is disabled if single-threaded applications write to a file using multiple communicators.

               Default: automatic

           romio_no_indep_rw
               Enables an optimization in which only the aggregators open the file, thus limiting the number of system open calls. For this hint to be valid, all I/O on the file must be done by MPI collective I/O calls
               (that is, no independent I/O) and collective buffering must not be disabled. Valid values are true or false.

               Default: false.

           striping_factor
               Specifies the number of Lustre file system stripes (stripe count) to assign to the file. This has no effect if the file already exists when the MPI_File_open() call is made. File striping cannot be changed
               after a file is created. Currently this hint applies only when MPICH_MPIIO_CB_ALIGN is set to 2.

               Default: the default value for the Lustre file system, or the value for the directory in which the file is created if the lfs setstripe command was used to set the stripe count of the directory to a value
               other than the system default.

           striping_unit
               Specifies in bytes the size of the Lustre file system stripes (stripe size) assigned to the file. This has no effect if the file already exists when the MPI_File_open() call is made. File striping cannot be
               changed after a file is created. Currently this hint applies only when MPICH_MPIIO_CB_ALIGN is set to 2.

               Default: the default value for the Lustre file system, or the value for the directory in which the file is created if the lfs setstripe command was used to set the stripe size of the directory to a value
               other than the system default.

       MPICH_MPIIO_HINTS_DISPLAY
           If enabled, causes rank 0 in the participating communicator to display the names and values of all MPI-IO hints that are set for the file being opened with the MPI_File_open call.

           Default: not enabled.

       MPICH_MPIIO_OFI_STARTUP_CONNECT
           By default, OFI connections between ranks are set up on demand. This allows for optimal performance while minimizing memory requirements. However, for MPIIO jobs requiring large number of PEs and IO aggregators,
           it may be beneficial to create OFI connections between PEs and IO aggregators in a coordinated manner at file open. If enabled, this feature will create connections between all ranks on each node and the IO
           aggregators in the job during MPI_File_open.

           Default: not enabled.

       MPICH_MPIIO_OFI_STARTUP_NODES_AGGREGATOR
           If MPICH_MPIIO_OFI_STARTUP_CONNECT is enabled, this specifies the number of nodes that will concurrently attempt to connect to each aggregator during the MPI_File_open. Increasing the value may be improve
           performance of the MPI_File_open in some configurations when MPICH_MPIIO_OFI_STARTUP_CONNECT is enabled.

           Default: 2

       MPICH_MPIIO_STATS
           If set to 1, a summary of file write and read access patterns is written by rank 0 to stderr. This information provides some insight into how I/O performance may be improved. The information is provided on a
           per-file basis and is written when the file is closed. It does not provide any timing information.

           If set to 2, a set of data files are written to the working directory, one file for each rank, with the filename prefix specified by the MPICH_MPIIO_STATS_FILE environment variable. The data is in
           comma-separated values (CSV) format, which can be summarized with the cray_mpiio_summary script in the /opt/cray/pe/mpich/version/ofi/mpich-cray/version*/bin* directory. Additional example scripts are provided
           in that directory to further process and display the data.

           Default: not set

       MPICH_MPIIO_STATS_FILE
           Specifies the filename prefix for the set of data files written when MPICH_MPIIO_STATS is set to 2. The filename prefix may be a full absolute pathname or a relative pathname.

           Summary plots of these files can be generated using the cray_mpiio_summary script from the /opt/cray/pe/mpich/version/ofi/mpich-cray/version*/bin* directory. Other example scripts for post-processing this data
           can also be found in /opt/cray/pe/mpich/version/ofi/mpich-cray/version*/bin*.

           Default: cray_mpiio_stats

       MPICH_MPIIO_STATS_INTERVAL_MSEC
           Specifies the time interval in milliseconds for each MPICH_MPIIO_STATS data point.

           Default: 250

       MPICH_MPIIO_TIMERS
           If set to 0 or not set at all, no timing data is collected.

           If set to 1, timing data for different phases in MPI_IO is collected locally by each MPI process. During MPI_FILE_close the data is consolidated and printed. Some timing data is displayed in seconds, other data
           is displayed in clock ticks, possibly scaled down. The relative values of the reported times are more important to the analysis than the absolute time. See also MPICH_MPIIO_TIMERS_SCALE

           More detailed information about MPI_IO performance can be obtained by using the MPICH_MPIIO_STATS feature and by using the CrayPat and Aprentice2 Timeline Report of I/O bandwidth.

           Default: 0

       MPICH_MPIIO_TIMERS_SCALE
           Specifies the power of 2 to use to scale the times reported by MPICH_MPIIO_TIMERS. The raw times are collected in clock ticks. This generally is a very large number and reducing all the times by the same scaling
           factor makes for a more compact display.

           If set to 0, or not set at all, MPI-IO automatically determines a scaling factor to limit the report times to 9 or fewer digits. This auto-determined value is displayed. To make run to run comparisons, you can
           set the scaling factor to your preferred value.

           Default: 0

       MPICH_MPIIO_TIME_WAITS
           If set to non-zero, time how long this rank has to wait for other ranks to catch up. This separates true metadata time from imbalance time.

           This is disabled when MPICH_MPIIO_TIMERS is not set. Otherwise it defaults to 1.

           Default: 1

CONFORMING TO
       This release of MPI derives from Argonne National Laboratory MPICH and implements the MPI-3.1 standard as documented by the MPI Forum in MPI: A Message Passing Interface Standard, Version 3.1, except as described in
       the NOTES section of this man page.

NOTES
   Using MPICH ABI Compatibility
       The primary focus of MPICH ABI Compatibility support is to support ISV (Independent Software Vendor) applications. Most ISV applications come with a set of MPI libraries. These MPI libraries may not work on a HPE
       system, but if they are ABI compatible with HPE Cray MPI one may be able to run the application using the high performance HPE libraries already installed on the system. Additional work may be required to get the
       ISV application to run depending on the particular launching scripts supplied with the ISV application. For more information please see the MPICH ABI Compatibility Status for Shasta white paper.

       To use MPICH ABI compatibility one must have a dynamically linked MPI program built with:

        1. A compiler that was supported by HPE at the time of compilation and that is compatible with a currently supported compiler.

        2. The OS used to build the MPI program must be compatible with the OS on the HPE system the program will be run on.

        3. A MPICH ABI compatible MPI implementation. See https://wiki.mpich.org/mpich/index.php/ABI_Compatibility_Initiative for a list of MPICH ABI compatible MPI implementations.

       Given all of the above requirements, one can run the application by doing the following:

        1. Make sure you have the programming environment module (PrgEnv-*) loaded for the compiler that was used to build the application. If necessary, change the compiler module to the version used to build the
           application or a version compatible with the compiler used to build the application.

        2. Load the cray-mpich-abi module. Make sure to swap with the normal cray-mpich module or unload the cray-mpich module before loading the cray-mpich-abi module.

        3. Launch the program with launcher supported by your HPE system. Note that one may have to translate launcher options, set (or not set) various environment variables, etc.

       A number of older ISV applications (built with Intel MPI versions previous to Intel MPI 5.0) may work with HPE Cray MPI. Some earlier Intel MPI versions are ABI compatible with the MPI library but have different
       library names/version numbers. These applications might run by loading the cray-mpich-abi-pre-intel5.0 module instead of the cray-mpich-abi module. All other previously mentioned caveats about running ISV
       applications apply.

SEE ALSO
       Cray Programming Environment User’s Guide

       MPICH ABI Compatibility Status for Shasta white paper

       cc(1), CC(1), ftn(1), intro_pmi(1)

  1                                                                                                       08/18/2021                                                                                              INTRO_MPI(3)
